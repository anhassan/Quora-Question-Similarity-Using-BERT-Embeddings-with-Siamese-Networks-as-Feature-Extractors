{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora Question Pairs Custom Project- Ganesh and Ahmad.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jagij11_0SU",
        "colab_type": "text"
      },
      "source": [
        "# Setup-Google Credentials to Download files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9T9uS4J_46Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1e2ae65a-3c19-4c4c-9634-6c8757460d94"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 2.9MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgS90tGG_wPg",
        "colab_type": "text"
      },
      "source": [
        "#Download Embeddings and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPOUDZWJ1Ci4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download all Required Files using Pydrive\n",
        "from numpy import genfromtxt\n",
        "link='https://drive.google.com/open?id=1qo15b31PARBro7vL3K4wUAhqKaB1D8m_'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('data.csv')  \n",
        "\n",
        "df_sub = pd.read_csv('data.csv')\n",
        "\n",
        "#Download all Required Files using Pydrive\n",
        "link='https://drive.google.com/open?id=1ygb657QqsYKYt1oVSM52qXAYb6V14sJe'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('quora_features_BERT.csv')\n",
        "features = pd.read_csv('quora_features_BERT.csv')\n",
        "\n",
        "\n",
        "#Embeddings3*2\n",
        "link='https://drive.google.com/open?id=1OUMTHgF5CkQcIEwNfxRmRE8R4XOpYlTs'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ber_qpair_balanced.csv')\n",
        "features_b = genfromtxt(\"ber_qpair_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "\n",
        "link='https://drive.google.com/open?id=18oaVUqneDcZ7Qf5zdhhazujFs4fVaENS'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('fastext_q1_balanced.csv')\n",
        "ft_emb_q1  = genfromtxt(\"fastext_q1_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "link='https://drive.google.com/open?id=10N03ESlX0NM9UDzbrMxSTuGIEzyBZ16Z'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('fastext_q2_balanced.csv')\n",
        "ft_emb_q2  = genfromtxt(\"fastext_q2_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "\n",
        "link='https://drive.google.com/open?id=13yUPFl9FDRORXCGrPpnGsTKF5Vcn-HSb'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('glove_q1_balanced.csv')\n",
        "glove_emb_q1 = genfromtxt(\"glove_q1_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "link='https://drive.google.com/open?id=1lS0jQA7ywuEK54Q-5FAXpmPYK_LBfira'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('glove_q2_balanced.csv')\n",
        "glove_emb_q2 = genfromtxt(\"glove_q2_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "\n",
        "link='https://drive.google.com/open?id=1E50i08MJXcMTV9iC7_OggCaBVWF0uq9_'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('w2vec_q1_balanced.csv')\n",
        "w2v_emb_q1 = genfromtxt(\"w2vec_q1_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "link='https://drive.google.com/open?id=1u83-aaKQVXDIo-elNlg9vnHmo1Q63YTF'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('w2vec_q2_balanced.csv')\n",
        "w2v_emb_q2 = genfromtxt(\"w2vec_q2_balanced.csv\",delimiter=',',skip_header=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5bWU6dr-0fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ft_emb_q1 = ft_emb_q1[:,1:]\n",
        "ft_emb_q2 = ft_emb_q2[:,1:]\n",
        "w2v_emb_q1 = w2v_emb_q1[:,1:]\n",
        "w2v_emb_q2 = w2v_emb_q2[:,1:]\n",
        "glove_emb_q1 = glove_emb_q1[:,1:]\n",
        "glove_emb_q2 = glove_emb_q2[:,1:]\n",
        "features = features.drop(columns = ['question1', 'question2', 'is_duplicate','jaccard_distance']).values\n",
        "features_b = features_b[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7heWZA1RDUHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "5f82b886-ec22-4ce3-9a15-4010ce67305d"
      },
      "source": [
        "print('Fastext:',ft_emb_q1.shape,\n",
        "ft_emb_q2.shape)\n",
        "\n",
        "print('Word2vec:',w2v_emb_q1.shape,\n",
        "w2v_emb_q2.shape)\n",
        "\n",
        "print('Glove:',glove_emb_q1.shape,\n",
        "glove_emb_q2.shape)\n",
        "\n",
        "print('BERT features:',features.shape)\n",
        "\n",
        "print('BERT CLS:',features_b.shape)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fastext: (40428, 324) (40428, 324)\n",
            "Word2vec: (40428, 324) (40428, 324)\n",
            "Glove: (40428, 324) (40428, 324)\n",
            "BERT features: (40428, 25)\n",
            "BERT CLS: (40428, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXsNb1Sn1Eq5",
        "colab_type": "text"
      },
      "source": [
        "# Load Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gHIQsB81IYS",
        "colab_type": "code",
        "outputId": "3ebf2f09-aa83-456a-a5c0-f86dd83dedff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import sys\n",
        "import os \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "# from bert_serving.client import BertClient\n",
        "\n",
        "from numpy import genfromtxt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, FastText\n",
        "# from fse.models import Sentence2Vec\n",
        "# # Make sure, that the fast version of fse is available!\n",
        "# from fse.models.sentence2vec import CY_ROUTINES\n",
        "# assert CY_ROUTINES\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras import callbacks\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "# import tensorflow_hub as hub\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, Activation, BatchNormalization, regularizers, Add, concatenate, Layer,Lambda\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "\n",
        "# Network Architecture\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv1D , MaxPooling1D, Flatten,Dense,Input,Lambda\n",
        "from keras.layers import LSTM, Concatenate, Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras import backend as K\n",
        "# import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "# from models import InferSent\n",
        "from keras import regularizers"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYL1Fye65Rjd",
        "colab_type": "text"
      },
      "source": [
        "# Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFqVAoxE5TbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_base_network_cnn(input_dimensions):\n",
        "\n",
        "    input  = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
        "    conv1  = Conv1D(filters=32,kernel_size=8,strides=1,activation = 'relu',name='conv1')(input)\n",
        "    b1 = BatchNormalization()(conv1)\n",
        "    d1 = Dropout(0.1)(b1)\n",
        "    \n",
        "    pool1  = MaxPooling1D(pool_size=1,strides=1,name='pool1')(d1)\n",
        "    d2  = Dropout(0.1)(pool1)\n",
        "    \n",
        "    conv2  = Conv1D(filters=64,kernel_size=6,strides=1,activation = 'relu',name='conv2')(d2)\n",
        "    b2 = BatchNormalization()(conv2)\n",
        "    d3 = Dropout(0.1)(b2)\n",
        "    \n",
        "    pool2  = MaxPooling1D(pool_size=1,strides=1,name='pool2')(d3)\n",
        "    d4 = Dropout(0.1)(pool2)\n",
        "    \n",
        "    conv3  = Conv1D(filters=128,kernel_size=4,strides=1,activation = 'relu',name='conv3')(d4)\n",
        "    b3 = BatchNormalization()(conv3)\n",
        "    d4 = Dropout(0.1)(b3)\n",
        "    \n",
        "    \n",
        "    pool3  = MaxPooling1D(pool_size=1,strides=1,name='pool3')(d4)\n",
        "    d5 = Dropout(0.1)(pool3)\n",
        "    \n",
        "    flat   = Flatten(name='flat_cnn')(d5)\n",
        "    d1 = Dense(100, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(flat)\n",
        "    drop1 = Dropout(0.1)(d1)\n",
        "    b1 = BatchNormalization()(drop1)\n",
        "    d2 = Dense(25,kernel_regularizer=regularizers.l2(0.01))(b1)\n",
        "    drop2 = Dropout(0.1)(d2)\n",
        "    b2 = BatchNormalization()(drop2)\n",
        "    d2 = Dense(5,kernel_regularizer=regularizers.l2(0.01))(b2)\n",
        "    drop3 = Dropout(0.1)(d2)\n",
        "    bn = BatchNormalization()(drop3)\n",
        "\n",
        "    model  = Model(input=input,output=bn)\n",
        "  \n",
        "  \n",
        "    return model\n",
        "\n",
        "\n",
        "def dense_network(features1):\n",
        "    input = Input(shape=(features1[0],))\n",
        "    #x = Flatten()(features)\n",
        "    d1 = Dense(100, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(input)\n",
        "    drop1 = Dropout(0.1)(d1)\n",
        "    b1 = BatchNormalization()(drop1)\n",
        "    d2 = Dense(25,kernel_regularizer=regularizers.l2(0.01))(b1)\n",
        "    drop2 = Dropout(0.1)(d2)\n",
        "    b2 = BatchNormalization()(drop2)\n",
        "    d2 = Dense(5,kernel_regularizer=regularizers.l2(0.01))(b2)\n",
        "    drop3 = Dropout(0.1)(d2)\n",
        "    b3 = BatchNormalization()(drop3)\n",
        "#     flat   = Flatten(name='flat_dnn2')(b3)\n",
        "    model = Model(input = input,output=b3)\n",
        "    return model\n",
        "  \n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "\n",
        "def create_network(input_dimensions,num_features):\n",
        "\n",
        "#     # #Fasttext\n",
        "    base_network_lstm_1 = dense_network([324,1])\n",
        "    input_a_lstm_1 = Input(shape=(input_dimensions[0],))\n",
        "    input_b_lstm_1 = Input(shape=(input_dimensions[0],))\n",
        "    # LSTM with embedding 1\n",
        "    inter_a_lstm_1 = base_network_lstm_1(input_a_lstm_1)\n",
        "    inter_b_lstm_1 = base_network_lstm_1(input_b_lstm_1)\n",
        "    d_lstm_1 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_1, inter_b_lstm_1])\n",
        "\n",
        "\n",
        "    #W2V\n",
        "    base_network_lstm_2 = dense_network([324,1])\n",
        "    input_a_lstm_2 = Input(shape=(input_dimensions[0],))\n",
        "    input_b_lstm_2 = Input(shape=(input_dimensions[0],))\n",
        "    # LSTM with embedding 2\n",
        "    inter_a_lstm_2 = base_network_lstm_2(input_a_lstm_2)\n",
        "    inter_b_lstm_2 = base_network_lstm_2(input_b_lstm_2)\n",
        "    d_lstm_2 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_2, inter_b_lstm_2])\n",
        "\n",
        "\n",
        "    #Glove\n",
        "    base_network_lstm_3 = dense_network([324,1])\n",
        "    input_a_lstm_3 = Input(shape=(input_dimensions[0],))\n",
        "    input_b_lstm_3 = Input(shape=(input_dimensions[0],))\n",
        "    # LSTM with embedding 3\n",
        "    inter_a_lstm_3 = base_network_lstm_3(input_a_lstm_3)\n",
        "    inter_b_lstm_3 = base_network_lstm_3(input_b_lstm_3)\n",
        "    d_lstm_3 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_3, inter_b_lstm_3])\n",
        "\n",
        "    #Uncomment to Use BERT as Siamese as well\n",
        "#     base_network_lstm_4 = dense_network([768,1])\n",
        "#     input_a_lstm_4 = Input(shape=(768,))\n",
        "#     input_b_lstm_4 = Input(shape=(768,))\n",
        "#     # LSTM with embedding 3\n",
        "#     inter_a_lstm_4 = base_network_lstm_4(input_a_lstm_4)\n",
        "#     inter_b_lstm_4 = base_network_lstm_4(input_b_lstm_4)\n",
        "\n",
        "\n",
        "#     CNN\n",
        "    base_network_cnn = create_base_network_cnn(input_dimensions)\n",
        "    # CNN with 3 channel embedding\n",
        "    input_a_cnn = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
        "    input_b_cnn = Input(shape=(input_dimensions[0],input_dimensions[1]))\n",
        "    inter_a_cnn = base_network_cnn(input_a_cnn)\n",
        "    inter_b_cnn = base_network_cnn(input_b_cnn)\n",
        "\n",
        "\n",
        "\n",
        "    d_cnn = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_cnn, inter_b_cnn])\n",
        "    d_lstm_1 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_1, inter_b_lstm_1])\n",
        "    d_lstm_2 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_2, inter_b_lstm_2])\n",
        "    d_lstm_3 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_3, inter_b_lstm_3])\n",
        "#     d_lstm_4 = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([inter_a_lstm_4, inter_b_lstm_4])\n",
        "\n",
        "    # Additional Features from (BERT)\n",
        "    features = Input(shape=(num_features,))\n",
        "\n",
        "    #BERT itself\n",
        "    features_b = Input(shape=(768,))\n",
        "\n",
        "\n",
        "    #Concatenation of Features\n",
        "    feature_set = Concatenate(axis=-1)([d_cnn,d_lstm_1,d_lstm_2,d_lstm_3,features,features_b])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #x = Flatten()(features)\n",
        "    d1 = Dense(300, activation='relu',kernel_regularizer=regularizers.l2(0.01))(feature_set)\n",
        "    drop1 = Dropout(0.1)(d1)\n",
        "    b1 = BatchNormalization()(drop1)\n",
        "    d2 = Dense(20, activation='relu',kernel_regularizer=regularizers.l2(0.001))(b1)\n",
        "    drop2 = Dropout(0.1)(d2)\n",
        "    b2 = BatchNormalization()(drop2)\n",
        "    d3 = Dense(2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(b2)\n",
        "\n",
        "\n",
        "#     model = Model(input=[feature_set,features_b], output=d3)\n",
        "    model = Model(input=[input_a_cnn, input_b_cnn , input_a_lstm_1, input_b_lstm_1, input_a_lstm_2, input_b_lstm_2, input_a_lstm_3, input_b_lstm_3,features,features_b], output=d3)  \n",
        "    print(\"Model Architecture Designed\")\n",
        "    return model\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIxjut-O6Ydm",
        "colab_type": "text"
      },
      "source": [
        "# Load Data and Embeddings from Downloaded files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUQ5m4ba6eYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sub['question1'] = df_sub['question1'].apply(lambda x: str(x))\n",
        "df_sub['question2'] = df_sub['question2'].apply(lambda x: str(x))\n",
        "q1sents = list(df_sub['question1'])\n",
        "q2sents = list(df_sub['question2'])\n",
        "tokenized_q1sents = [word_tokenize(i) for i in list(df_sub['question1'])]\n",
        "tokenized_q2sents = [word_tokenize(i) for i in list(df_sub['question2'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGI1etv57Uqh",
        "colab_type": "text"
      },
      "source": [
        "# Prepare-Train-Test-Validation Data for Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXKplIhu7AcA",
        "colab_type": "code",
        "outputId": "b5363f2b-9069-4b44-a3f4-6d25e8cce191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "num_train = int(df_sub.shape[0] * 0.70)\n",
        "num_val = int(df_sub.shape[0] * 0.10)\n",
        "num_test = df_sub.shape[0] - num_train - num_val \n",
        "\n",
        "print(\"Number of training pairs: %i\"%(num_train))\n",
        "print(\"Number of Validation pairs: %i\"%(num_val))\n",
        "print(\"Number of testing pairs: %i\"%(num_test))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training pairs: 28299\n",
            "Number of Validation pairs: 4042\n",
            "Number of testing pairs: 8087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFdqODaB7Cej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init data data arrays\n",
        "X_train_cnn_a = np.zeros([num_train, 324, 3])\n",
        "X_test_cnn_a  = np.zeros([num_test, 324, 3])\n",
        "X_val_cnn_a  = np.zeros([num_val, 324, 3])\n",
        "\n",
        "X_train_cnn_b = np.zeros([num_train, 324, 3])\n",
        "X_test_cnn_b  = np.zeros([num_test, 324, 3])\n",
        "X_val_cnn_b  = np.zeros([num_val, 324, 3])\n",
        "\n",
        "Y_train = np.zeros([num_train]) \n",
        "Y_test = np.zeros([num_test])\n",
        "Y_val = np.zeros([num_val]) \n",
        "\n",
        "\n",
        "#Labels\n",
        "Y_train = df_sub['is_duplicate'].values[num_train]\n",
        "Y_val = df_sub['is_duplicate'].values[num_val]\n",
        "Y_test = df_sub['is_duplicate'].values[num_val]\n",
        "\n",
        "\n",
        "\n",
        "num_val = num_train + int(df_sub.shape[0] * 0.10)\n",
        "# fill data arrays with features\n",
        "X_train_cnn_a[:,:,0] = ft_emb_q1[:num_train]\n",
        "X_train_cnn_a[:,:,1] = w2v_emb_q1[:num_train]\n",
        "X_train_cnn_a[:,:,2] = glove_emb_q1[:num_train]\n",
        "# X_train_cnn_a[:,:,3] = bert_q1_pca[:num_train]\n",
        "\n",
        "X_train_cnn_b[:,:,0] = ft_emb_q2[:num_train]\n",
        "X_train_cnn_b[:,:,1] = w2v_emb_q2[:num_train]\n",
        "X_train_cnn_b[:,:,2] = glove_emb_q2[:num_train]\n",
        "# X_train_cnn_b[:,:,3] = bert_q2_pca[:num_train]\n",
        "\n",
        "features_train = features[:num_train]\n",
        "features_b_train = features_b[:num_train]\n",
        "Y_train = df_sub[:num_train]['is_duplicate'].values\n",
        "\n",
        "X_val_cnn_a[:,:,0] = ft_emb_q1[num_train:num_val]\n",
        "X_val_cnn_a[:,:,1] = w2v_emb_q1[num_train:num_val]\n",
        "X_val_cnn_a[:,:,2] = glove_emb_q1[num_train:num_val]\n",
        "# X_val_cnn_a[:,:,3] = bert_q1_pca[num_train:num_val]\n",
        "\n",
        "X_val_cnn_b[:,:,0] = ft_emb_q2[num_train:num_val]\n",
        "X_val_cnn_b[:,:,1] = w2v_emb_q2[num_train:num_val]\n",
        "X_val_cnn_b[:,:,2] = glove_emb_q2[num_train:num_val]\n",
        "# X_val_cnn_b[:,:,3] = bert_q2_pca[num_train:num_val]\n",
        "\n",
        "features_val = features[num_train:num_val]\n",
        "features_b_val =features_b[num_train:num_val]\n",
        "Y_val = df_sub[num_train:num_val]['is_duplicate'].values\n",
        "\n",
        "\n",
        "X_test_cnn_a[:,:,0] = ft_emb_q1[num_val:]\n",
        "X_test_cnn_a[:,:,1] = w2v_emb_q1[num_val:]\n",
        "X_test_cnn_a[:,:,2] = glove_emb_q1[num_val:]\n",
        "# X_test_cnn_a[:,:,3] = bert_q1_pca[num_val:]\n",
        "\n",
        "X_test_cnn_b[:,:,0] = ft_emb_q2[num_val:]\n",
        "X_test_cnn_b[:,:,1] = w2v_emb_q2[num_val:]\n",
        "X_test_cnn_b[:,:,2] = glove_emb_q2[num_val:]\n",
        "# X_test_cnn_b[:,:,3] = bert_q2_pca[num_val:]\n",
        "\n",
        "\n",
        "features_test = features[num_val:]\n",
        "features_b_test = features_b[num_val:]\n",
        "Y_test = df_sub[num_val:]['is_duplicate'].values\n",
        "\n",
        "\n",
        "Y_train_old = Y_train\n",
        "Y_test_old = Y_test\n",
        "Y_val_old = Y_val\n",
        "\n",
        "Y_train = keras.utils.to_categorical(Y_train, num_classes=2)\n",
        "Y_test = keras.utils.to_categorical(Y_test, num_classes=2)\n",
        "Y_val = keras.utils.to_categorical(Y_val, num_classes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xETTzV37HLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "70df95e2-5288-4565-ed6c-8df8100cde38"
      },
      "source": [
        "# # Training Sets for Siamese1\n",
        "X_intera_train_1 = X_train_cnn_a[:,:,0]\n",
        "X_interb_train_1 = X_train_cnn_b[:,:,0]\n",
        "X_train_lstm1_a = X_intera_train_1\n",
        "X_train_lstm1_b = X_interb_train_1\n",
        "\n",
        "X_intera_val_1 = X_val_cnn_a[:,:,0]\n",
        "X_interb_val_1 = X_val_cnn_b[:,:,0]\n",
        "X_val_lstm1_a = X_intera_val_1\n",
        "X_val_lstm1_b = X_interb_val_1\n",
        "\n",
        "X_intera_test_1 = X_test_cnn_a[:,:,0]\n",
        "X_interb_test_1 = X_test_cnn_b[:,:,0]\n",
        "X_test_lstm1_a = X_intera_test_1\n",
        "X_test_lstm1_b = X_interb_test_1\n",
        "\n",
        "\n",
        "# Sets for Siamese2\n",
        "X_intera_train_2 = X_train_cnn_a[:,:,1]\n",
        "X_interb_train_2 = X_train_cnn_b[:,:,1]\n",
        "X_train_lstm2_a = X_intera_train_2\n",
        "X_train_lstm2_b = X_interb_train_2\n",
        "\n",
        "X_intera_val_2 = X_val_cnn_a[:,:,1]\n",
        "X_interb_val_2 = X_val_cnn_b[:,:,1]\n",
        "X_val_lstm2_a = X_intera_val_2\n",
        "X_val_lstm2_b = X_interb_val_2\n",
        "\n",
        "X_intera_test_2 = X_test_cnn_a[:,:,1]\n",
        "X_interb_test_2 = X_test_cnn_b[:,:,1]\n",
        "X_test_lstm2_a = X_intera_test_2\n",
        "X_test_lstm2_b = X_interb_test_2\n",
        "\n",
        "# Set for Siamese3\n",
        "\n",
        "X_intera_train_3 = X_train_cnn_a[:,:,2]\n",
        "X_interb_train_3 = X_train_cnn_b[:,:,2]\n",
        "X_train_lstm3_a = X_intera_train_3\n",
        "X_train_lstm3_b = X_interb_train_3\n",
        "\n",
        "X_intera_val_3 = X_val_cnn_a[:,:,2]\n",
        "X_interb_val_3 = X_val_cnn_b[:,:,2]\n",
        "X_val_lstm3_a = X_intera_val_3\n",
        "X_val_lstm3_b = X_interb_val_3\n",
        "X_intera_test_3 = X_test_cnn_a[:,:,2]\n",
        "X_interb_test_3 = X_test_cnn_b[:,:,2]\n",
        "X_test_lstm3_a = X_intera_test_3\n",
        "X_test_lstm3_b = X_interb_test_3\n",
        "\n",
        "\n",
        "# Test Set for LSTM4 BERT\n",
        "\n",
        "# X_intera_train_4 = bert_q1[:num_train]\n",
        "# X_train_lstm4_a = X_intera_train_4\n",
        "\n",
        "# X_intera_val_4 = bert_q1[num_train:num_val]\n",
        "# X_val_lstm4_a = X_intera_val_4\n",
        "\n",
        "# X_intera_test_4 = bert_q1[num_val:]\n",
        "# X_test_lstm4_a = X_intera_test_4\n",
        "\n",
        "# X_interb_train_4 = bert_q2[:num_train]\n",
        "# X_train_lstm4_b = X_interb_train_4\n",
        "\n",
        "# X_interb_val_4 = bert_q2[num_train:num_val]\n",
        "# X_val_lstm4_b = X_interb_val_4\n",
        "\n",
        "# X_interb_test_4 = bert_q2[num_val:]\n",
        "# X_test_lstm4_b = X_interb_test_4\n",
        "# X_test_lstm4_b = X_interb_test_4\n",
        "\n",
        "print(\"Input Shapes\")\n",
        "print(\"CNN Shape\")\n",
        "print(X_train_cnn_a.shape,X_val_cnn_a.shape,X_test_cnn_a.shape)\n",
        "print(\"LSTM (x3) Shape:\")\n",
        "print(X_train_lstm1_a.shape,X_val_lstm1_a.shape,X_test_lstm1_a.shape)\n",
        "# print(\"LSTM (BERT) Shape:\")\n",
        "# print(X_train_lstm4_a.shape,X_val_lstm4_a.shape,X_test_lstm4_a.shape)\n",
        "\n",
        "print(\"Features shape:\",features_train.shape,features_val.shape,features_test.shape)\n",
        "print(\"BERT Features shape:\",features_b_train.shape,features_b_val.shape,features_b_test.shape)\n",
        "\n",
        "print(\"Labels Shape\")\n",
        "print(Y_train.shape,Y_val.shape,Y_test.shape)\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Shapes\n",
            "CNN Shape\n",
            "(28299, 324, 3) (4042, 324, 3) (8087, 324, 3)\n",
            "LSTM (x3) Shape:\n",
            "(28299, 324) (4042, 324) (8087, 324)\n",
            "Features shape: (28299, 25) (4042, 25) (8087, 25)\n",
            "BERT Features shape: (28299, 768) (4042, 768) (8087, 768)\n",
            "Labels Shape\n",
            "(28299, 2) (4042, 2) (8087, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFTgzbX67aHM",
        "colab_type": "text"
      },
      "source": [
        "# Training with DNN as Classification Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpHsSI3R7gt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "003ca741-38d8-48e0-bcc3-e614cf792713"
      },
      "source": [
        "net = create_network([324,3],25)\n",
        "optimizer = Adam(lr=0.001)\n",
        "net.compile(loss=\"binary_crossentropy\", optimizer=optimizer,metrics=['accuracy'])\n",
        "print(net.summary())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n",
            "W0807 07:59:20.282669 140614001997696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:151: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
            "W0807 07:59:23.125292 140614001997696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0807 07:59:23.132891 140614001997696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0807 07:59:23.141194 140614001997696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Architecture Designed\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, 324, 3)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           (None, 324, 3)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           (None, 324)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           (None, 324)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           (None, 324)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_21 (InputLayer)           (None, 324)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_23 (InputLayer)           (None, 324)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_24 (InputLayer)           (None, 324)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_9 (Model)                 (None, 5)            4005419     input_26[0][0]                   \n",
            "                                                                 input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "model_6 (Model)                 (None, 5)            35675       input_17[0][0]                   \n",
            "                                                                 input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "model_7 (Model)                 (None, 5)            35675       input_20[0][0]                   \n",
            "                                                                 input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "model_8 (Model)                 (None, 5)            35675       input_23[0][0]                   \n",
            "                                                                 input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 1)            0           model_9[1][0]                    \n",
            "                                                                 model_9[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 1)            0           model_6[1][0]                    \n",
            "                                                                 model_6[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 1)            0           model_7[1][0]                    \n",
            "                                                                 model_7[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 1)            0           model_8[1][0]                    \n",
            "                                                                 model_8[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_28 (InputLayer)           (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           (None, 768)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 797)          0           lambda_8[0][0]                   \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 lambda_10[0][0]                  \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 input_28[0][0]                   \n",
            "                                                                 input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 300)          239400      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 300)          0           dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 300)          1200        dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 20)           6020        batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 20)           0           dense_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 20)           80          dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 2)            42          batch_normalization_32[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 4,359,186\n",
            "Trainable params: 4,357,058\n",
            "Non-trainable params: 2,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDZZOS5O7hd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"./Model7_{epoch:02d}_{val_acc:.4f}.h5\"\n",
        "checkpoint = callbacks.ModelCheckpoint(filepath, \n",
        "                                    monitor='val_acc', \n",
        "                                    verbose=0, \n",
        "                                    save_best_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "net.fit([ X_train_cnn_a, X_train_cnn_b,X_train_lstm1_a, X_train_lstm1_b,\n",
        "            X_train_lstm2_a, X_train_lstm2_b,X_train_lstm3_a, X_train_lstm3_b,features_train,features_b_train], \n",
        "            Y_train,\n",
        "          validation_data=([X_val_cnn_a, X_val_cnn_b,X_val_lstm1_a, X_val_lstm1_b,\n",
        "                          X_val_lstm2_a, X_val_lstm2_b,X_val_lstm3_a, X_val_lstm3_b,features_val,features_b_val]\n",
        "                          , Y_val),\n",
        "          batch_size=384, nb_epoch=20, shuffle=True,callbacks = callbacks_list)\n",
        "\n",
        "score = net.evaluate([X_test_cnn_a, X_test_cnn_b,X_test_lstm1_a, X_test_lstm1_b,\n",
        "              X_test_lstm2_a, X_test_lstm2_b,X_test_lstm3_a, X_test_lstm3_b,features_test,features_b_test],Y_test,batch_size=384)\n",
        "\n",
        "print('Test loss : {:.4f}'.format(score[0]))\n",
        "print('Test accuracy : {:.4f}'.format(score[1]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gKYW5v47k9k",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Best Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bc1lC-N7hby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2571489e-0cd5-40fe-89ac-f5b2f3205ef6"
      },
      "source": [
        "#Download all Required Files using Pydrive\n",
        "link='https://drive.google.com/open?id=1mnDiANiu_m16IdWZdGewKKUiqGkFSjbx'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Model1_40_0.7984.h5')\n",
        "model = load_model('Model1_40_0.7984.h5')\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer,metrics=['acc'])\n",
        "\n",
        "score = model.evaluate([X_test_cnn_a, X_test_cnn_b,X_test_lstm1_a, X_test_lstm1_b,\n",
        "              X_test_lstm2_a, X_test_lstm2_b,X_test_lstm3_a, X_test_lstm3_b,features_test,features_b_test],Y_test,batch_size=128)\n",
        "print('Test loss : {:.4f}'.format(score[0]))\n",
        "print('Test accuracy : {:.4f}'.format(score[1]))\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8087/8087 [==============================] - 29s 4ms/step\n",
            "Test loss : 0.5678\n",
            "Test accuracy : 0.8018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6vlIPgy7oce",
        "colab_type": "text"
      },
      "source": [
        "# Classification with SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1ElyZIE7haA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Getting features for classical machine learning algorithms\n",
        "def feature_extractor():\n",
        "  intermediate_layer_model = Model(inputs=net.input,\n",
        "                                 outputs=net.layers[-8].output)\n",
        "  data_train = [ X_train_cnn_a, X_train_cnn_b,X_train_lstm1_a, X_train_lstm1_b,\n",
        "            X_train_lstm2_a, X_train_lstm2_b,X_train_lstm3_a, X_train_lstm3_b,features_train,features_b_train]\n",
        "\n",
        "  data_validation = [X_val_cnn_a, X_val_cnn_b,X_val_lstm1_a, X_val_lstm1_b,\n",
        "                          X_val_lstm2_a, X_val_lstm2_b,X_val_lstm3_a, X_val_lstm3_b,features_val,features_b_val]\n",
        "\n",
        "  data_test = [X_test_cnn_a, X_test_cnn_b,X_test_lstm1_a, X_test_lstm1_b,\n",
        "              X_test_lstm2_a, X_test_lstm2_b,X_test_lstm3_a, X_test_lstm3_b,features_test,features_b_test]\n",
        "\n",
        "\n",
        "  intermediate_output_train = intermediate_layer_model.predict(data_train)\n",
        "  intermediate_output_validation = intermediate_layer_model.predict(data_validation)\n",
        "  intermediate_output_test = intermediate_layer_model.predict(data_test)\n",
        "  \n",
        "  features_combined = [intermediate_output_train, intermediate_output_validation,intermediate_output_test]\n",
        "  return features_combined\n",
        "\n",
        "[feats_train,feats_val,feats_test] = feature_extractor()\n",
        "\n",
        "# Getting data for classical machine learning algorithms\n",
        "\n",
        "\n",
        "train_feats = np.array(list(feats_train) + list(feats_val))\n",
        "train_Y     = np.array(list(Y_train_old) + list(Y_val_old))\n",
        "\n",
        "test_feats = feats_test\n",
        "test_Y = Y_test_old\n",
        "\n",
        "print(\"train_feats :\",train_feats.shape)\n",
        "print(\"train_Y : \",train_Y.shape)\n",
        "\n",
        "print(\"test_feats : \",test_feats.shape)\n",
        "print(\"test_Y : \",test_Y.shape)\n",
        "\n",
        "# Support Vector Machine\n",
        "from sklearn.svm import SVC\n",
        "svm_clf = SVC(gamma='auto')\n",
        "svm_clf.fit(train_feats,train_Y)\n",
        "print(svm_clf.score(test_feats,test_Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmDLd16G7sio",
        "colab_type": "text"
      },
      "source": [
        "# Classification with XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaTzwukQ7vVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model_xg = XGBClassifier()\n",
        "model_xg.fit(train_feats, train_Y)\n",
        "y_pred_xg = model.predict(test_feats)\n",
        "predictions_xg = [round(value) for value in y_pred_xg]\n",
        "accuracy = accuracy_score(test_feats, predictions_xg)\n",
        "\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}